# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import streamlit as st
import google.generativeai as genai

# --- ì´ˆê¸° ì„¤ì • ---

# 2. API í‚¤ ì„¤ì •
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    API_KEY = "AIzaSyBZD2AqxEMJTStEm3UXdjaloS-Mjf9-GgE"

genai.configure(api_key=API_KEY)

# --- AI ì—­í•  ë° ì •ë³´ ì„¤ì • ---

system_instruction = """
ë„ˆëŠ” 'ì„œì¼ëŒ€í•™êµ' í•™ìƒë“¤ì„ ìœ„í•œ AI ì±—ë´‡ 'ì„œì¼ë¹„ì„œ'ì•¼. í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•´.
ì•„ë˜ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì •ë³´ê°€ ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì¤˜. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ì¤˜.

[ì„œì¼ëŒ€í•™êµ í•µì‹¬ ì •ë³´]
- **ìœ„ì¹˜**: ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬ ìš©ë§ˆì‚°ë¡œ90ê¸¸ 28 (ë©´ëª©ë™)
- **êµí†µ**:
    - **ì§€í•˜ì² **: 7í˜¸ì„  ë©´ëª©ì—­ 2ë²ˆ ì¶œêµ¬, 7í˜¸ì„  ì‚¬ê°€ì •ì—­ 1ë²ˆ ì¶œêµ¬
    - **ì…”í‹€ë²„ìŠ¤**:
        - **ë©´ëª©ì—­**: 2ë²ˆ ì¶œêµ¬ ì• 'íŒŒë¦¬ë°”ê²Œëœ¨' ì•ì—ì„œ íƒ‘ìŠ¹ (ì˜¤ì „ 8ì‹œë¶€í„° ìˆ˜ì‹œ ìš´í–‰)
        - **ì‚¬ê°€ì •ì—­**: 1ë²ˆ ì¶œêµ¬ ì• 'ë¡¯ë°ë¦¬ì•„' ì•ì—ì„œ íƒ‘ìŠ¹ (ì˜¤ì „ 8ì‹œë¶€í„° ìˆ˜ì‹œ ìš´í–‰)
- **ì£¼ìš” í•™ê³¼**:
    - **ITê³„ì—´**: ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼, AIìœµí•©ì½˜í…ì¸ í•™ê³¼, ì •ë³´ë³´í˜¸í•™ê³¼, ì»´í“¨í„°ì „ìê³µí•™ê³¼ ë“±
    - **ë””ìì¸ê³„ì—´**: ì‹¤ë‚´ë””ìì¸í•™ê³¼, ì‹œê°ë””ìì¸í•™ê³¼, íŒ¨ì…˜ì‚°ì—…í•™ê³¼, ì˜í™”ë°©ì†¡ê³µì—°ì˜ˆìˆ í•™ê³¼ ë“±
    - **ì¸ë¬¸ì‚¬íšŒê³„ì—´**: ìœ ì•„êµìœ¡ê³¼, ì‚¬íšŒë³µì§€í•™ê³¼, ì„¸ë¬´íšŒê³„í•™ê³¼, ë¹„ì¦ˆë‹ˆìŠ¤ì¼ë³¸ì–´ê³¼, ì¤‘êµ­ì–´ë¬¸í™”í•™ê³¼ ë“±
    - **ê¸°íƒ€**: ììœ¨ì „ê³µí•™ê³¼, ê°„í˜¸í•™ê³¼, ìƒëª…í™”í•™ê³µí•™ê³¼ ë“±
- **ì—°ë½ì²˜**:
    - **ëŒ€í‘œ ë²ˆí˜¸**: 02-490-7300
    - **ì…í•™ ë¬¸ì˜**: 02-490-7331~3
- **íŠ¹ì§•**: 'ì§€ë•ë°°ì–‘, ì´ˆì§€ì¼ê´€'ì„ êµí›ˆìœ¼ë¡œ ì‚¼ê³  ìˆìœ¼ë©°, ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ì „ë¬¸ ì¸ì¬ ì–‘ì„±ì„ ëª©í‘œë¡œ í•¨.
"""

# --- ì›¹ UI ì„¤ì • ---

st.set_page_config(
    page_title="ì„œì¼ëŒ€í•™êµ AI ì±—ë´‡",
    page_icon="ğŸ“"
)

# í™”ë©´ì„ ë‘ ê°œì˜ ë‹¨(column)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
# ë¹„ìœ¨ì„ [1, 3]ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì™¼ìª½ ë‹¨(ë¡œê³ )ë³´ë‹¤ ì˜¤ë¥¸ìª½ ë‹¨(ì œëª©)ì´ 3ë°° ë” ë„“ê²Œ ë§Œë“­ë‹ˆë‹¤.
st.markdown("""
    <style>
           .block-container {
                padding-top: 2rem;
            }
    </style>
    """, unsafe_allow_html=True)

col1, col2 = st.columns([1, 3])

# ì™¼ìª½ ë‹¨(col1)ì— ë¡œê³  ì´ë¯¸ì§€ë¥¼ ë„£ìŠµë‹ˆë‹¤.
with col1:
    st.markdown(
        """
          <div style="margin-top: -rem; margin-left: -55rem;">
            <a href="https://www.seoil.ac.kr/">
                <img src="https://ncs.seoil.ac.kr/GateWeb/Common/images/login/%EC%84%9C%EC%9D%BC%EB%8C%80%20%EB%A1%9C%EA%B3%A0.png" width="200">
            </a>
        <div>
        """,
        unsafe_allow_html=True,
    )

# ì˜¤ë¥¸ìª½ ë‹¨(col2)ì— ì œëª©ê³¼ ì„¤ëª…ì„ ë„£ìŠµë‹ˆë‹¤.
with col2:
    # ì œëª©ì˜ ì„¸ë¡œ ìœ„ì¹˜ë¥¼ ë¡œê³ ì™€ ë§ì¶”ê¸° ìœ„í•´ ì•½ê°„ì˜ ì—¬ë°±(padding)ì„ ì¶”ê°€
    st.markdown("<h1 style='padding-top: 1rem; margin-left: -10rem;'>ğŸ“ ì„œì¼ëŒ€í•™êµ AI ì±—ë´‡ 'ì„œì¼ë¹„ì„œ'</h1>", unsafe_allow_html=True)


st.write("ì•ˆë…•í•˜ì„¸ìš”! ì„œì¼ëŒ€í•™êµì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")
st.write("ì˜ˆì‹œ: ì…”í‹€ë²„ìŠ¤ëŠ” ì–´ë””ì„œ íƒ€? / ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼ì— ëŒ€í•´ ì•Œë ¤ì¤˜")


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    model = genai.GenerativeModel('gemini-1.5-flash')
    
    chat_history_for_api = [
        {'role': 'user', 'parts': [system_instruction]},
        {'role': 'model', 'parts': ["ë„¤, ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì„œì¼ëŒ€í•™êµ ì•ˆë‚´ AI 'ì„œì¼ë¹„ì„œ'ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"]}
    ]
    for msg in st.session_state.messages:
        role = 'user' if msg['role'] == 'user' else 'model'
        chat_history_for_api.append({'role': role, 'parts': [msg['content']]})

    chat_session = model.start_chat(history=chat_history_for_api)

    with st.spinner("ì„œì¼ë¹„ì„œê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
        response = chat_session.send_message(prompt)
        ai_response = response.text

    st.session_state.messages.append({"role": "model", "content": ai_response})
    with st.chat_message("model"):
        st.markdown(ai_response)