import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import chromadb

# ë³¸ì¸ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
API_KEY = "AIzaSyD_BbwVdQZfH71Fez8gyDQlW09BbbY15VM"
genai.configure(api_key=API_KEY)

def scrape_and_process_page(topic, url):
    """
    í˜ì´ì§€ì˜ ì£¼ì œ(topic)ì— ë”°ë¼ ìµœì í™”ëœ ë°©ë²•ìœ¼ë¡œ ìŠ¤í¬ë ˆì´í•‘ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # 1. ëª¨ë“  í˜ì´ì§€ì˜ ê¸°ë³¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ìŠµë‹ˆë‹¤.
    main_content = soup.find(id='_contentBuilder')

    if main_content:
        print(f" INFO: '{topic}' í˜ì´ì§€ì—ì„œ id='_contentBuilder' ë‚´ìš©ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return main_content.get_text(separator='\n', strip=True)
    else:
        print(f"  -> WARN: '{topic}' í˜ì´ì§€ì—ì„œ id='_contentBuilder' ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
        return soup.get_text(separator='\n', strip=True)
        
        
def prepare_and_save_embeddings():
    print("ì„œì¼ëŒ€í•™êµ í™ˆí˜ì´ì§€ ì •ë³´ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘...")
    urls = {
        "í•™ì‚¬ê³µì§€": "https://www.seoil.ac.kr/seoil/599/subview.do",
        "ê³µì§€ì‚¬í•­": "https://www.seoil.ac.kr/seoil/598/subview.do",
        "í–‰ì‚¬ì•ˆë‚´": "https://www.seoil.ac.kr/seoil/600/subview.do",
        "í™ë³´ì‚¬í•­": "https://www.seoil.ac.kr/seoil/602/subview.do",
        "ì…”í‹€ë²„ìŠ¤": "https://www.seoil.ac.kr/seoil/520/subview.do",
        "ì„œì¼ëŒ€í•™êµ": "https://www.seoil.ac.kr/sites/seoil/index.do",
        "í•™êµì†Œì‹": "https://www.seoil.ac.kr/seoil/616/subview.do",
        "ìŠ¤í„°ë””ê³µê°„": "https://www.seoil.ac.kr/seoil/583/subview.do",
        "PCì´ìš©, VRì‹¤": "https://www.seoil.ac.kr/seoil/584/subview.do",
        "í¸ì˜ì , ì¹´í˜": "https://www.seoil.ac.kr/seoil/585/subview.do",
        "í•™ìƒì‹ë‹¹": "https://www.seoil.ac.kr/seoil/3896/subview.do",
        "íœ´ê²Œê³µê°„": "https://www.seoil.ac.kr/seoil/586/subview.do",
        "í¸ì˜ì‹œì„¤": "https://www.seoil.ac.kr/seoil/587/subview.do",
        "ì²´ìœ¡ì‹œì„¤": "https://www.seoil.ac.kr/seoil/588/subview.do",
        "ëŒ€í•™ìƒí™œë©”ë‰´ì–¼": "https://www.seoil.ac.kr/seoil/3409/subview.do",
        "ì°¾ì•„ì˜¤ì‹œëŠ”ê¸¸": "https://www.seoil.ac.kr/seoil/520/subview.do",
        "ë„ì„œê´€": "https://www.seoil.ac.kr/seoil/580/subview.do"
    }
    
    all_chunks = []
    for topic, url in urls.items():
        try:
            text = scrape_and_process_page(topic, url)
            if not text.strip():
                print(f"  -> WARN: '{topic}' í˜ì´ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # í…ìŠ¤íŠ¸ë¥¼ 500ì ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            for i in range(0, len(text), 500):
                chunk = text[i:i+500]
                all_chunks.append({"topic": topic, "content": chunk})
            print(f"âœ… '{topic}' í˜ì´ì§€ ìŠ¤í¬ë ˆì´í•‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ '{topic}' í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    print("\ní…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ì„ë² ë”©í•˜ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    if not all_chunks:
        print("ìŠ¤í¬ë ˆì´í•‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ì„ë² ë”©ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    

    contents = [chunk['content'] for chunk in all_chunks]

    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±
    # (ChromaDBëŠ” ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ìë™ìœ¼ë¡œ ì €ì¥/ê´€ë¦¬í•´ì¤ë‹ˆë‹¤)
    db_path = "./chroma_db"
    client = chromadb.PersistentClient(path=db_path)
    collection = client.get_or_create_collection(name="seoil_info_db") # DB ì´ë¦„ ì§€ì •
    
    embedding_result = genai.embed_content(model="models/embedding-001", content=contents, task_type="RETRIEVAL_DOCUMENT")

    # ChromaDBì— ì €ì¥í•  ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
    documents = contents
    embeddings = embedding_result['embedding']
    ids = [f"chunk_{i}" for i in range(len(documents))] # ê° ì •ë³´ ì¡°ê°ì˜ ê³ ìœ  ID

    try:
        if collection.count() > 0:
            print(f"ê¸°ì¡´ DB({collection.count()}ê°œ)ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            client.delete_collection(name="seoil_info_db")
            collection = client.get_or_create_collection(name="seoil_info_db")
    except Exception:
        pass

    # ë°ì´í„°ë¥¼ ChromaDBì— ì¶”ê°€
    collection.add(
        embeddings=embeddings,
        documents=documents,
        ids=ids
    )
        
    print(f"\nğŸ‰ ì„ë² ë”© ì™„ë£Œ! ì´ {len(documents)}ê°œì˜ ì •ë³´ ì¡°ê°ì´ '{db_path}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
if __name__ == "__main__":
    prepare_and_save_embeddings()