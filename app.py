# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import streamlit as st
import google.generativeai as genai
import numpy as np
import pickle

# --- ì´ˆê¸° ì„¤ì • ---

# 2. API í‚¤ ì„¤ì •
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    API_KEY = "AIzaSyBZD2AqxEMJTStEm3UXdjaloS-Mjf9-GgE"

genai.configure(api_key=API_KEY)

# ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(RAG) ê¸°ëŠ¥, ì˜ë¯¸ ê²€ìƒ‰(Semanric Search) ê¸°ëŠ¥ êµ¬í˜„
# 3. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_resource(show_spinner="ì‚¬ì „ í•™ìŠµëœ í•™êµ ì •ë³´ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
def load_vector_store():
    """ì €ì¥ëœ vector_store.pkl íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        with open("vector_store.pkl", "rb") as f:
            return pickle.load(f)
    except FileNotFoundError:
        st.error("'vector_store.pkl' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'prepare_data.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None


# 4. ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜
def find_relevant_info(query, vector_store, top_k=5):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ê³ , ì €ì¥ëœ ì •ë³´ë“¤ ì¤‘ì—ì„œ ì˜ë¯¸ìƒ ê°€ì¥ ìœ ì‚¬í•œ ì •ë³´ ì¡°ê° top_kê°œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    if not vector_store:
        return ""
        
    # ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©
    query_embedding = genai.embed_content(model="models/embedding-001", content=query, task_type="RETRIEVAL_QUERY")['embedding']
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for item in vector_store:
        similarity = np.dot(query_embedding, item['embedding']) / (np.linalg.norm(query_embedding) * np.linalg.norm(item['embedding']))
        similarities.append(similarity)
    
    # ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ top_kê°œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    # ê´€ë ¨ ì •ë³´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    relevant_info = "\n\n".join([vector_store[i]['content'] for i in top_indices])
    return relevant_info

# ì•± ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë”© ë° ì„ë² ë”© ì‹¤í–‰
vector_store = load_vector_store()

# --- AI ì—­í•  ë° ì •ë³´ ì„¤ì • ---
system_instruction = """
ë„ˆëŠ” 'ì„œì¼ëŒ€í•™êµ' í•™ìƒë“¤ì„ ìœ„í•œ AI ì±—ë´‡ 'ì„œì¼ë¹„ì„œ'ì•¼. í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•´.
ê¸°ì¡´ì— ë‹µë³€ ê°€ëŠ¥í•œ ë²”ìœ„ì˜ ì§ˆë¬¸ì„ ìš°ì„ ì ìœ¼ë¡œ ë‹µë³€í•˜ê³  ì •í™•í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì£¼ì–´ì§„ [ì°¸ê³  ì •ë³´]ì™€ [ì´ì „ ëŒ€í™” ë‚´ìš©]ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.
ì°¸ê³  ì •ë³´ì—ë„ ë‚´ìš©ì´ ì—†ë‹¤ë©´, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì¤˜.
"""

st.set_page_config(page_title="ì„œì¼ëŒ€í•™êµ AI ì±—ë´‡", page_icon="ğŸ“")
st.markdown("""
    <style>
           .block-container {
                padding-top: 2rem;
            }
           .fixed-logo { position: fixed; top: 2.5rem; left: 0.7rem; z-index: 99; }
    </style>
    <div class="fixed-logo">
        <a href="https://www.seoil.ac.kr/"><img src="https://ncs.seoil.ac.kr/GateWeb/Common/images/login/%EC%84%9C%EC%9D%BC%EB%8C%80%20%EB%A1%9C%EA%B3%A0.png" width="200"></a>
    </div>
    """, unsafe_allow_html=True)
st.markdown("""
    <div style="text-align: center;">
        <h2>ğŸ“ ì„œì¼ëŒ€í•™êµ AI ì±—ë´‡ 'ì„œì¼ë¹„ì„œ'</h2>
        <p>ì•ˆë…•í•˜ì„¸ìš”! ì„œì¼ëŒ€í•™êµì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.</p>
    </div>
    """, unsafe_allow_html=True)
st.write("")

# 5. ì„¸ì…˜ ìƒíƒœì— ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# 6. ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 7. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì˜ë¯¸ ê¸°ë°˜(ì˜ë¯¸ ê²€ìƒ‰_Semanric Sarch)ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    with st.spinner("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘..."):
        retrieved_info = find_relevant_info(prompt, vector_store)

    # --- [ ë””ë²„ê¹… ì½”ë“œ ] --- ë””ë²„ê¹…ì‹œ #7ì˜ ì½”ë“œ ì£¼ì„ì²˜ë¦¬
    #with st.spinner("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘..."):
     #    retrieved_info = find_relevant_info(prompt, vector_store)

    # AIì—ê²Œ ì „ë‹¬ë  ì°¸ê³  ì •ë³´ë¥¼ í™”ë©´ì— ì •ë³´(info) ë°•ìŠ¤ë¡œ í‘œì‹œ
    #if retrieved_info:
     #   st.info(f"**[AI ì°¸ê³  ì •ë³´]**\n\n---\n\n{retrieved_info}")
    #else:
     #   st.error("**[AI ì°¸ê³  ì •ë³´]**\n\n---\n\nê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    # --- [ ë””ë²„ê¹… ì½”ë“œ ] ---

    # ì´ì „ ëŒ€í™” ë‚´ìš© í˜•ì‹í™”
    previous_conversation = "\n".join([f'{msg["role"]}: {msg["content"]}' for msg in st.session_state.messages])

    # AIì—ê²Œ ì „ë‹¬í•  ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    final_prompt = f"""
[ì°¸ê³  ì •ë³´]
{retrieved_info if retrieved_info else "ê°€ì ¸ì˜¨ ì •ë³´ ì—†ìŒ"}

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{previous_conversation}

[ì‚¬ìš©ì ì§ˆë¬¸]
{prompt}
"""
    
    model = genai.GenerativeModel('gemini-flash-latest')
    chat_session = model.start_chat(history=[{'role': 'user', 'parts': [system_instruction]}])

    with st.chat_message("model"):
        # 1. stream=True ì˜µì…˜ì„ ì œê±°í•˜ì—¬ ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ API í˜¸ì¶œ
        response = chat_session.send_message(final_prompt)

        # 2. ì‘ë‹µ ê°ì²´(response)ì—ì„œ .textë¥¼ ì‚¬ìš©í•´ "ìˆœìˆ˜ í…ìŠ¤íŠ¸"ë§Œ ì¶”ì¶œ
        ai_response = response.text

        # 3. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— í‘œì‹œ
        st.markdown(ai_response)

    # 4. ê¹¨ë—í•˜ê²Œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
    st.session_state.messages.append({"role": "model", "content": ai_response})